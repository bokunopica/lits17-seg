{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as tt\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToDeviceLoader:\n",
    "    def __init__(self,data,device):\n",
    "        self.data = data\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for batch in self.data:\n",
    "            yield to_device(batch,self.device)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "def to_device(data,device):\n",
    "    if isinstance(data,(list,tuple)):\n",
    "        return [to_device(x,device) for x in data]\n",
    "    return data.to(device,non_blocking=True)\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPER_PARAMETER\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device_ids = [0]\n",
    "NUM_CLASSES = 3\n",
    "DATA_DIR = \"/home/qianq/data/lung_png\"\n",
    "device = get_device()\n",
    "BATCH_SIZE = 1\n",
    "CROP_SIZE = (512, 512) # 在代码不修改的情况 crop 调整为最小值\n",
    "LEARNING_RATE = 5e-5\n",
    "NUM_EPOCHS = 2\n",
    "SEG_COLORMAP = [\n",
    "    [0, 0, 0], \n",
    "    [255, 255, 255], \n",
    "    [255, 106, 106], \n",
    "]\n",
    "SEG_CLASSES = [\n",
    "    'background', \n",
    "    'lung_left', \n",
    "    'lung_right'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def fname_to_ids(fname):\n",
    "#     \"\"\"\n",
    "#     return ct_id and ct_slice_id\n",
    "#     volume-001-058.png -> 1, 58\n",
    "#     \"\"\"\n",
    "#     seg_list = fname.split('-')\n",
    "#     return int(seg_list[1]), int(seg_list[2].replace('.png', ''))\n",
    "\n",
    "# mode = torchvision.io.image.ImageReadMode.RGB\n",
    "# data_dir = DATA_DIR\n",
    "# source_list = os.listdir(os.path.join(DATA_DIR, 'source'))\n",
    "# for fname in source_list[137:]:\n",
    "#     source = torchvision.io.read_image(os.path.join(data_dir, 'source', fname), mode)\n",
    "#     ct_id, slice_id = fname_to_ids(fname)\n",
    "#     target_fname_lung_left = \"seg-lung-left-%03d-%03d.png\" % (ct_id, slice_id)\n",
    "#     target_fname_lung_right = \"seg-lung-right-%03d-%03d.png\" % (ct_id, slice_id)\n",
    "#     label_left = torchvision.io.read_image(os.path.join(data_dir, 'target', target_fname_lung_left), mode)\n",
    "#     label_right = torchvision.io.read_image(os.path.join(data_dir, 'target', target_fname_lung_right), mode)\n",
    "#     if (\n",
    "#         source.shape != torch.Size([3, 512, 512])\n",
    "#         or label_left.shape != torch.Size([3, 512, 512])\n",
    "#         or label_right.shape != torch.Size([3, 512, 512])\n",
    "#     ):\n",
    "#         print(fname)\n",
    "#         print(source.shape)\n",
    "#         print(label_left.shape)\n",
    "#         print(label_right.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullConvolutionalNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pretrained_net = nn.Sequential(\n",
    "            *list(\n",
    "                torchvision.models.resnet18(weights=\"IMAGENET1K_V1\").children()\n",
    "            )[:-2]\n",
    "        )\n",
    "        self.final_conv = nn.Conv2d(512, NUM_CLASSES, kernel_size=1)\n",
    "        self.transpose_conv = nn.ConvTranspose2d(\n",
    "            NUM_CLASSES, \n",
    "            NUM_CLASSES,\n",
    "            kernel_size=64, \n",
    "            padding=16, \n",
    "            stride=32,\n",
    "        )\n",
    "        W = self.bilinear_kernel(NUM_CLASSES, NUM_CLASSES, 64)\n",
    "        self.transpose_conv.requires_grad_(False)\n",
    "        self.transpose_conv.weight.copy_(W)\n",
    "        self.transpose_conv.requires_grad_(True)\n",
    "        self._model = nn.Sequential(\n",
    "            self.pretrained_net,\n",
    "            self.final_conv,\n",
    "            self.transpose_conv,\n",
    "        )\n",
    "    \n",
    "    def bilinear_kernel(self, in_channels, out_channels, kernel_size):\n",
    "        factor = (kernel_size + 1) // 2\n",
    "        if kernel_size % 2 == 1:\n",
    "            center = factor - 1\n",
    "        else:\n",
    "            center = factor - 0.5\n",
    "        og = (torch.arange(kernel_size).reshape(-1, 1),\n",
    "            torch.arange(kernel_size).reshape(1, -1))\n",
    "        filt = (1 - torch.abs(og[0] - center) / factor) * \\\n",
    "            (1 - torch.abs(og[1] - center) / factor)\n",
    "        weight = torch.zeros((in_channels, out_channels,\n",
    "                            kernel_size, kernel_size))\n",
    "        weight[range(in_channels), range(out_channels), :, :] = filt\n",
    "        return weight\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LungSegDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"A customized dataset to load the LitsLiverDataset`\"\"\"\n",
    "\n",
    "    normalize_transform = tt.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    resize_transform = tt.Resize(CROP_SIZE)\n",
    "    \n",
    "\n",
    "    def __init__(self, is_train, crop_size, data_dir, object='lung'):\n",
    "        \"\"\"\n",
    "        object[choice]:\n",
    "            lung_left\n",
    "            lung_right\n",
    "            all\n",
    "        \"\"\"\n",
    "        self.transform = self.normalize_transform\n",
    "        self.crop_size = crop_size\n",
    "        self.is_train = is_train\n",
    "        self.data_dir = data_dir\n",
    "        self.object = object\n",
    "\n",
    "        self.source_images = os.listdir(os.path.join(data_dir, 'source'))\n",
    "        self.train_source_list = []\n",
    "        self.test_source_list = []\n",
    "        for fname in self.source_images:\n",
    "            ct_id, slice_id = self.fname_to_ids(fname)\n",
    "            ct_id = int(ct_id)\n",
    "            if ct_id <= 100:\n",
    "                self.train_source_list.append(fname)\n",
    "            elif ct_id > 100:\n",
    "                self.test_source_list.append(fname)\n",
    "        self.colormap2label = self.get_colormap2label()\n",
    "\n",
    "    @classmethod\n",
    "    def normalize_image(cls, img):\n",
    "        return cls.normalize_transform(img.float() / 255)\n",
    "\n",
    "    def filter(self, imgs):\n",
    "        return [img for img in imgs if (\n",
    "            img.shape[1] >= self.crop_size[0] and\n",
    "            img.shape[2] >= self.crop_size[1])]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature, label = self.read_single_image(\n",
    "            object=self.object, \n",
    "            idx=idx,\n",
    "        )\n",
    "        feature = self.normalize_image(feature)\n",
    "        feature, label = self.rand_crop(feature, label,*self.crop_size)\n",
    "        return (feature, self.label_indices(label, self.colormap2label))\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.is_train:\n",
    "            return len(self.train_source_list)\n",
    "        else:\n",
    "            return len(self.test_source_list)\n",
    "\n",
    "    def read_single_image(self, idx, object='lung'):\n",
    "        mode = torchvision.io.image.ImageReadMode.RGB\n",
    "        if self.is_train:\n",
    "            source_images_list = self.train_source_list\n",
    "        else:\n",
    "            source_images_list = self.test_source_list\n",
    "        fname = source_images_list[idx]\n",
    "        ct_id, slice_id = self.fname_to_ids(fname)\n",
    "        ct_id = int(ct_id)\n",
    "        slice_id = int(slice_id)\n",
    "        feature = torchvision.io.read_image(\n",
    "            os.path.join(self.data_dir, 'source', fname),\n",
    "            mode,\n",
    "        )\n",
    "        target_fname_lung_left = \"seg-lung-left-%03d-%03d.png\" % (ct_id, slice_id)\n",
    "        target_fname_lung_right = \"seg-lung-right-%03d-%03d.png\" % (ct_id, slice_id)\n",
    "\n",
    "        if object in ['lung_left','lung_right']:\n",
    "            target_fname = target_fname_lung_left if object == 'lung_left' else target_fname_lung_right\n",
    "            label = torchvision.io.read_image(os.path.join(self.data_dir, 'target', target_fname), mode)\n",
    "        else:\n",
    "            # both lung\n",
    "            label_lung_left = torchvision.io.read_image(os.path.join(self.data_dir, 'target', target_fname_lung_left), mode)\n",
    "            label_lung_right = torchvision.io.read_image(os.path.join(self.data_dir, 'target', target_fname_lung_right), mode)\n",
    "            # print(label_lung_left.max())\n",
    "            # print(label_lung_right.max())\n",
    "            label_idx = label_lung_right[0] == 255\n",
    "            label_lung_right[1][label_idx] = 106\n",
    "            label_lung_right[2][label_idx] = 106\n",
    "            label = label_lung_left + label_lung_right\n",
    "        # print(feature.shape, label.shape)\n",
    "        feature = self.resize_transform(feature)\n",
    "        label = self.resize_transform(label)\n",
    "        return feature, label\n",
    "\n",
    "    # @classmethod\n",
    "    # def read_images(cls, data_dir, object='liver', is_train=True):\n",
    "    #     \"\"\"\n",
    "    #     Read all Lits feature and liver-label images on the memory\n",
    "    #     [warning] it may cause machine crash\n",
    "    #     object[choice]:\n",
    "    #         - liver\n",
    "    #         - tumor\n",
    "    #     is_train:\n",
    "    #         - True: first 100 ct samples\n",
    "    #         - False: last 30 ct samples\n",
    "    #     \"\"\"\n",
    "    #     mode = torchvision.io.image.ImageReadMode.RGB\n",
    "    #     features, labels = [], []\n",
    "    #     images = os.listdir(os.path.join(data_dir, 'source'))\n",
    "    #     pbar = tqdm(total=len(images))\n",
    "    #     for i, fname in enumerate(images):\n",
    "    #         ct_id, slice_id = cls.fname_to_ids(fname)\n",
    "    #         ct_id = int(ct_id)\n",
    "    #         slice_id = int(slice_id)\n",
    "    #         if is_train and ct_id > 100:\n",
    "    #             continue\n",
    "    #         elif (not is_train) and ct_id <= 100:\n",
    "    #             continue\n",
    "    #         features.append(torchvision.io.read_image(\n",
    "    #             os.path.join(data_dir, 'source', fname), mode)\n",
    "    #         )\n",
    "    #         if object == 'liver':\n",
    "    #             target_fname = \"seg-liver-%03d-%03d.png\" % (ct_id, slice_id)\n",
    "    #         else:\n",
    "    #             target_fname = \"seg-tumor-%03d-%03d.png\" % (ct_id, slice_id)\n",
    "    #         label = torchvision.io.read_image(os.path.join(data_dir, 'target', target_fname), mode)\n",
    "    #         labels.append(label)\n",
    "    #         pbar.update(1)\n",
    "    #     return features, labels\n",
    "\n",
    "    @classmethod\n",
    "    def fname_to_ids(cls, fname):\n",
    "        \"\"\"\n",
    "        return ct_id and ct_slice_id\n",
    "        volume-001-058.png -> 1, 58\n",
    "        \"\"\"\n",
    "        seg_list = fname.split('-')\n",
    "        return seg_list[1], seg_list[2].replace('.png', '')\n",
    "\n",
    "    def get_colormap2label(self):\n",
    "        \"\"\"Build the mapping from RGB to class indices for labels.\n",
    "\n",
    "        Defined in :numref:`sec_semantic_segmentation`\"\"\"\n",
    "        colormap2label = torch.zeros(256 ** 3, dtype=torch.long)\n",
    "        for i, colormap in enumerate(SEG_COLORMAP):\n",
    "            colormap2label[\n",
    "                (colormap[0] * 256 + colormap[1]) * 256 + colormap[2]] = i\n",
    "        return colormap2label\n",
    "\n",
    "    def rand_crop(self, feature, label, height, width):\n",
    "        \"\"\"Randomly crop both feature and label images.\n",
    "\n",
    "        Defined in :numref:`sec_semantic_segmentation`\"\"\"\n",
    "        rect = torchvision.transforms.RandomCrop.get_params(\n",
    "            feature, (height, width))\n",
    "        feature = torchvision.transforms.functional.crop(feature, *rect)\n",
    "        label = torchvision.transforms.functional.crop(label, *rect)\n",
    "        return feature, label\n",
    "\n",
    "    def label_indices(self, colormap, colormap2label):\n",
    "        \"\"\"Map any RGB values in labels to their class indices.\n",
    "\n",
    "        Defined in :numref:`sec_semantic_segmentation`\"\"\"\n",
    "        colormap = colormap.permute(1, 2, 0).numpy().astype('int32')\n",
    "        idx = ((colormap[:, :, 0] * 256 + colormap[:, :, 1]) * 256\n",
    "               + colormap[:, :, 2])\n",
    "        return colormap2label[idx]\n",
    "\n",
    "    @classmethod\n",
    "    def label2image(cls, pred, device):\n",
    "        colormap = torch.tensor(SEG_COLORMAP, device=device)\n",
    "        X = pred.long()\n",
    "        return colormap[X, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = LungSegDataset(True, (512,512), DATA_DIR, 'lung')\n",
    "# source, target = ds[76]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    ''' Computes and stores the average and current value '''\n",
    "    def __init__(self) -> None:\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.val = 0.0\n",
    "        self.avg = 0.0\n",
    "        self.sum = 0.0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val: float, n: int = 1) -> None:\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "\n",
    "def get_dataloader_workers():\n",
    "    \"\"\"Use 4 processes to read the data.\"\"\"\n",
    "    return 4\n",
    "\n",
    "def load_data_lits(batch_size, crop_size):\n",
    "    \"\"\"Load the LITS liver semantic segmentation dataset.\n",
    "\n",
    "    Defined in :numref:`sec_semantic_segmentation`\"\"\"\n",
    "    data_dir = DATA_DIR\n",
    "    num_workers = get_dataloader_workers()\n",
    "    \n",
    "    train_iter = torch.utils.data.DataLoader(\n",
    "        LungSegDataset(True, crop_size, data_dir, object='all'), batch_size,\n",
    "        shuffle=True, drop_last=True, num_workers=num_workers)\n",
    "    test_iter = torch.utils.data.DataLoader(\n",
    "        LungSegDataset(False, crop_size, data_dir, object='all'), batch_size,\n",
    "        drop_last=True, num_workers=num_workers)\n",
    "    return train_iter, test_iter\n",
    "\n",
    "# def loss(inputs, targets):\n",
    "#     return F.cross_entropy(inputs, targets, reduction='none').mean(1).mean(1)\n",
    "\n",
    "reduce_sum = lambda x, *args, **kwargs: x.sum(*args, **kwargs)\n",
    "argmax = lambda x, *args, **kwargs: x.argmax(*args, **kwargs)\n",
    "astype = lambda x, *args, **kwargs: x.type(*args, **kwargs)\n",
    "\n",
    "\n",
    "def pixel_accuracy(y_hat, y):\n",
    "    \"\"\"Compute the number of correct predictions.[each pixel]\n",
    "\n",
    "    Defined in :numref:`sec_softmax_scratch`\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = argmax(y_hat, axis=1)\n",
    "    cmp = astype(y_hat, y.dtype) == y\n",
    "    size = 1\n",
    "    for s in cmp.shape:\n",
    "        size *= s\n",
    "    return float(cmp.sum()/size)\n",
    "\n",
    "\n",
    "def calc_iou(pred, target, n_classes = 2):\n",
    "    #n_classes ：the number of classes in your dataset\n",
    "    ious = []\n",
    "    pred = argmax(pred, axis=1)\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "\n",
    "    # Ignore IoU for background class (\"0\")\n",
    "    for cls in range(1, n_classes):  # This goes from 1:n_classes-1 -> class \"0\" is ignored\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = (pred_inds[target_inds]).long().sum().data.cpu().item()  # Cast to long to prevent overflows\n",
    "        union = pred_inds.long().sum().data.cpu().item() + target_inds.long().sum().data.cpu().item() - intersection\n",
    "        # if union == 0:\n",
    "        #     ious.append(float('nan'))  # If there is no ground truth, do not include in evaluation\n",
    "        # else:\n",
    "        #     ious.append(float(intersection) / float(max(union, 1)))\n",
    "        if union != 0:\n",
    "            ious.append([float(intersection), float(max(union, 1))])\n",
    "        \n",
    "    # return np.array(ious)\n",
    "    return ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = load_data_lits(BATCH_SIZE, CROP_SIZE)\n",
    "train_dl = ToDeviceLoader(train_iter, device)\n",
    "test_dl = ToDeviceLoader(test_iter, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FullConvolutionalNet()\n",
    "model = model.to(device)\n",
    "# model = torch.nn.DataParallel(model, device_ids=device_ids)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, LEARNING_RATE, eta_min=0)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, LEARNING_RATE, total_iters=NUM_EPOCHS)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, max_lr=LEARNING_RATE, epochs=NUM_EPOCHS, steps_per_epoch=len(train_dl)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_iter, model, criterion, optimizer, epoch, total_epochs, lr_scheduler):\n",
    "    loss_metric = AverageMeter()\n",
    "    acc_metric = AverageMeter()\n",
    "    num_steps = len(train_iter)\n",
    "    lr_rate = lr_scheduler.get_last_lr()\n",
    "    pbar = tqdm(total=num_steps)\n",
    "    pbar.set_description(f\"[Train] EPOCH [{epoch}/{total_epochs}], lr=[{lr_rate}]:\")\n",
    "    for i, (x, y) in enumerate(train_iter):\n",
    "        batch_cnt = x.shape[0]\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        # print(x.shape)\n",
    "        # print(y.shape)\n",
    "        # print(pred.shape)\n",
    "        l = criterion(pred, y)\n",
    "        l.sum().backward()\n",
    "        train_loss_sum = l.sum()\n",
    "        train_acc_sum = pixel_accuracy(pred, y)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        loss_metric.update(float(train_loss_sum), n=batch_cnt)\n",
    "        acc_metric.update(train_acc_sum, n=1)\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix(loss=loss_metric.avg, acc=acc_metric.avg)\n",
    "    return loss_metric.avg, acc_metric.avg\n",
    "\n",
    "\n",
    "def valid(test_iter, model, criterion, epoch, total_epochs, lr_scheduler):\n",
    "    loss_metric = AverageMeter()\n",
    "    acc_metric = AverageMeter()\n",
    "    num_steps = len(test_iter)\n",
    "    lr_rate = lr_scheduler.get_last_lr()\n",
    "    pbar = tqdm(total=num_steps)\n",
    "    pbar.set_description(f\"[Valid] EPOCH [{epoch}/{total_epochs}], lr=[{lr_rate}]:\")\n",
    "    for i, (x, y) in enumerate(test_iter):\n",
    "        batch_cnt = x.shape[0]\n",
    "        model.eval()\n",
    "        pred = model(x)\n",
    "        l = criterion(pred, y)\n",
    "        train_loss_sum = l.sum()\n",
    "        train_acc_sum = pixel_accuracy(pred, y)\n",
    "        loss_metric.update(float(train_loss_sum), n=batch_cnt)\n",
    "        acc_metric.update(train_acc_sum, n=1)\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix(loss=loss_metric.avg, acc=acc_metric.avg)\n",
    "    return loss_metric.avg, acc_metric.avg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] EPOCH [1/2], lr=[[2.5e-09]]::   0%|          | 0/3427 [00:00<?, ?it/s]/home/qianq/mycodes/lits17-seg/venv/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/qianq/mycodes/lits17-seg/venv/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/qianq/mycodes/lits17-seg/venv/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "[Train] EPOCH [1/2], lr=[[2.5e-09]]::   1%|          | 31/3427 [00:01<01:17, 43.90it/s, acc=0.727, loss=0.736]/home/qianq/mycodes/lits17-seg/venv/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "[Train] EPOCH [1/2], lr=[[2.5e-09]]::  21%|██        | 727/3427 [00:12<00:45, 58.79it/s, acc=0.97, loss=0.0953] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m \u001b[39m*\u001b[39m \u001b[39m50\u001b[39m)\n\u001b[1;32m      4\u001b[0m lr_scheduler\u001b[39m.\u001b[39mget_last_lr()\n\u001b[0;32m----> 5\u001b[0m train_loss, train_acc \u001b[39m=\u001b[39m train(train_dl, model, criterion, optimizer, epoch, NUM_EPOCHS, lr_scheduler)\n\u001b[1;32m      6\u001b[0m valid_loss, valid_acc \u001b[39m=\u001b[39m valid(test_dl, model, criterion, epoch, NUM_EPOCHS, lr_scheduler)\n\u001b[1;32m      7\u001b[0m history\u001b[39m.\u001b[39mappend([epoch, train_loss, train_acc, valid_loss, valid_acc])\n",
      "Cell \u001b[0;32mIn[12], line 22\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_iter, model, criterion, optimizer, epoch, total_epochs, lr_scheduler)\u001b[0m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m lr_scheduler\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> 22\u001b[0m loss_metric\u001b[39m.\u001b[39mupdate(\u001b[39mfloat\u001b[39;49m(train_loss_sum), n\u001b[39m=\u001b[39mbatch_cnt)\n\u001b[1;32m     23\u001b[0m acc_metric\u001b[39m.\u001b[39mupdate(train_acc_sum, n\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     24\u001b[0m pbar\u001b[39m.\u001b[39mupdate(\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] EPOCH [1/2], lr=[[2.5e-09]]::  21%|██        | 727/3427 [00:29<00:45, 58.79it/s, acc=0.97, loss=0.0953]"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    print('-' * 50)\n",
    "    lr_scheduler.get_last_lr()\n",
    "    train_loss, train_acc = train(train_dl, model, criterion, optimizer, epoch, NUM_EPOCHS, lr_scheduler)\n",
    "    valid_loss, valid_acc = valid(test_dl, model, criterion, epoch, NUM_EPOCHS, lr_scheduler)\n",
    "    history.append([epoch, train_loss, train_acc, valid_loss, valid_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_imgs(imgs):\n",
    "    row_cnt = len(imgs)\n",
    "    col_cnt = len(imgs[0])\n",
    "    plt.figure(figsize=(6,8)) #设置窗口大小\n",
    "    plt.suptitle('lung seg') # 图片名称\n",
    "    i = 0\n",
    "    for (source, pred, target) in imgs:\n",
    "        s_idx = i*col_cnt + 1\n",
    "        p_idx = s_idx + 1\n",
    "        t_idx = p_idx + 1\n",
    "        plt.subplot(row_cnt, col_cnt, s_idx), plt.title('source')\n",
    "        plt.imshow(source), plt.axis('off')\n",
    "        plt.subplot(row_cnt, col_cnt, p_idx), plt.title('pred')\n",
    "        plt.imshow(pred), plt.axis('off') #这里显示灰度图要加cmap\n",
    "        plt.subplot(row_cnt, col_cnt, t_idx), plt.title('target')\n",
    "        plt.imshow(target), plt.axis('off')\n",
    "        i = i+1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = LungSegDataset(False, CROP_SIZE, DATA_DIR, object='all')\n",
    "idx1 = ds.test_source_list.index(\"volume-120-191.png\")\n",
    "idx2 = ds.test_source_list.index(\"volume-129-228.png\")\n",
    "idx_list = [idx1, idx1+500, idx2, idx2+50]\n",
    "n = 4\n",
    "X = torch.zeros((n, 3, 512, 512))\n",
    "Y = torch.zeros((n, 512, 512))\n",
    "\n",
    "for i in range(n):\n",
    "    X[i], Y[i] = ds[idx_list[i]]\n",
    "\n",
    "y_hat = model(X).argmax(dim=1)\n",
    "# print(X.shape)\n",
    "imgs = []\n",
    "\n",
    "for i in range(n):\n",
    "    x = X[i].permute(1, 2, 0).to('cpu')\n",
    "    pred = LungSegDataset.label2image(y_hat[i], device).to('cpu')\n",
    "    y = LungSegDataset.label2image(Y[i], device).to('cpu')\n",
    "    imgs.append([x, pred, y])\n",
    "\n",
    "show_imgs(imgs)\n",
    "# print(imgs[0].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
